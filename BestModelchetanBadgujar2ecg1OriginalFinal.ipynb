{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nfn3daXcoIn0"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_IDJ5W1jUJM"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haV1bIBFoHSi"
      },
      "outputs": [],
      "source": [
        "# !pip install -q tensorflow tensorflow-addons scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "from scipy.io import arff\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "06s-D4UGYzcY"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "#Todo 1.0\n",
        "#!wget -O /content/ECG5000_train.pickle /content/sample_data/ECG5000_train (5).pickle\n",
        "\n",
        "with open(\"/content/sample_data/ECG5000_train (5).pickle\", 'rb') as f:\n",
        "  ecg_train = pickle.load(f)\n",
        "#!wget -O /content/ECG5000_train.pickle /content/sample_data/ECG5000_validation (3).pickle\n",
        "\n",
        "with open(\"/content/sample_data/ECG5000_validation (3).pickle\", 'rb') as f:\n",
        "  ecg_validation = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKwqQ8FzjbS2"
      },
      "source": [
        "# Checking data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQRYl6B2jkil"
      },
      "source": [
        "# Feature label split and standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QTcMoc2DlNFk"
      },
      "outputs": [],
      "source": [
        "swapTrainvld = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nurk7AuOgXom"
      },
      "source": [
        "# Data imbalance.\n",
        "\n",
        "Here we try to increase minority class samples with few of the techniques.\n",
        "Noise plus minus\n",
        "\n",
        "1.   Plus Minus noise: Adding or substracting small noise from the row to get more sample. Idea is that it will help capture underlying patterns of minority class.  \n",
        "2.   Smothing odd even: Similar to animation industry where animators draw around 12 frames for the animaiton of video 24frames per second. As drawing all frames would make animation shake a lot and unstable. Similar concept was used here to smooth out samples and make more samples from it self.\n",
        "This works similar to\n",
        "<br>Sample = [1,2,4,2,5,1]\n",
        "<br>oddsample = [1,1,4,4,5, 5]\n",
        "<br>evensample = [2,2,2,2,1,1]<br>\n",
        "This way are are able to get 3 samples from one original one. The example many not be as good but it is similar in working. It can also cause side effects like missing out peaks and valleys in sample. But it has worked little better then making model heavily baised towards minority samples.\n",
        "For example, if majority class had 100 samples, we would need around 700 duplicates of minority samples to get good f1 compared to raw.\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oMnc2MSZO74J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def augment_minority_with_noise_and_smoothing(x_train, y_train, classes_to_augment=[2, 3, 4], noise_std=0.05, target_ratio=1.3,\n",
        "                           dual_noise=True, use_smoothing=True, shuffle=True, random_state=42):\n",
        "\n",
        "    y_train = y_train.squeeze().astype(np.int32)\n",
        "    class_counts = np.bincount(y_train)\n",
        "    max_class_count = int(np.max(class_counts) * target_ratio)\n",
        "\n",
        "    x_aug = [x_train]\n",
        "    y_aug = [y_train]\n",
        "\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    for cls in classes_to_augment:\n",
        "        if cls >= len(class_counts):\n",
        "            continue\n",
        "\n",
        "        count = class_counts[cls]\n",
        "        if count >= max_class_count:\n",
        "            continue\n",
        "\n",
        "        n_needed = max_class_count - count\n",
        "        x_cls = x_train[y_train == cls]\n",
        "\n",
        "        new_samples = []\n",
        "        new_labels = []\n",
        "\n",
        "        while len(new_samples) < n_needed:\n",
        "            for sample in x_cls:\n",
        "                # plus minus noise\n",
        "                noise = rng.normal(0, noise_std, size=sample.shape)\n",
        "                new_samples.append(sample + noise)\n",
        "                new_labels.append(cls)\n",
        "                if dual_noise:\n",
        "                    new_samples.append(sample - noise)\n",
        "                    new_labels.append(cls)\n",
        "\n",
        "                # odd even smoothing\n",
        "                if use_smoothing:\n",
        "                    even_smoothed = sample.copy()\n",
        "                    odd_smoothed = sample.copy()\n",
        "\n",
        "                    # Repeating even values into odd positions\n",
        "                    even_smoothed[1::2] = even_smoothed[::2][:len(even_smoothed[1::2])]\n",
        "                    new_samples.append(even_smoothed)\n",
        "                    new_labels.append(cls)\n",
        "\n",
        "                    # Repeating odd values into even positions\n",
        "                    if len(sample) > 1:\n",
        "                        odd_smoothed[::2] = odd_smoothed[1::2][:len(odd_smoothed[::2])]\n",
        "                        new_samples.append(odd_smoothed)\n",
        "                        new_labels.append(cls)\n",
        "\n",
        "                if len(new_samples) >= n_needed:\n",
        "                    break\n",
        "\n",
        "        x_aug.append(np.array(new_samples[:n_needed]))\n",
        "        y_aug.append(np.array(new_labels[:n_needed]))\n",
        "\n",
        "    x_final = np.concatenate(x_aug, axis=0)\n",
        "    y_final = np.concatenate(y_aug, axis=0)\n",
        "\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(y_final))\n",
        "        rng.shuffle(indices)\n",
        "        x_final = x_final[indices]\n",
        "        y_final = y_final[indices]\n",
        "\n",
        "    return x_final, y_final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Li3_lZkNC1"
      },
      "source": [
        "This is just used for printing data types, shapes, and unique values for debugging purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V8U4-r9GfODT"
      },
      "outputs": [],
      "source": [
        "def printInfo(X_trn, y_trn, mesage = \"Trn\"):\n",
        "  print(\"\\n\")\n",
        "\n",
        "  try:\n",
        "    if y_trn.shape[1] > 1:\n",
        "      counts = np.sum(y_trn, axis=0)\n",
        "      uniques = np.arange(y_trn.shape[1])\n",
        "    else:\n",
        "      print(\"Y is not one hot encoded here\")\n",
        "      uniques, counts = np.unique(y_trn.flatten(), return_counts = True)\n",
        "  except:\n",
        "    print(\"Y is not one hot encoded here\")\n",
        "    uniques, counts = np.unique(y_trn.flatten(), return_counts = True)\n",
        "  print(mesage)\n",
        "  print(\"Types: \")\n",
        "  print(f\"\\tX_trn type = {type(X_trn)}, y_trn type = {type(y_trn)}\")\n",
        "  print(\"Shapes: \")\n",
        "  print(f\"\\tX_trn.shape = {X_trn.shape}, y_trn.shape = {y_trn.shape}\")\n",
        "  print(\"Y unique plus counts\")\n",
        "  for i,j in zip(uniques, counts):\n",
        "    print(f\"\\tClass = {i}, counts = {j}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1sANohpjw6G"
      },
      "source": [
        "# Data agumentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XClNuY-kfEJ"
      },
      "source": [
        "Method for duplicating minority samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3dN48QcqqOY_"
      },
      "outputs": [],
      "source": [
        "def augment_class(X, y, target_class, augment_count):\n",
        "    class_indices = np.where(y == target_class)[0]\n",
        "    chosen = np.random.choice(class_indices, augment_count, replace=True)\n",
        "    return X[chosen], y[chosen]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFVEAkIskk9u"
      },
      "source": [
        "# Data prep\n",
        "This method applies all data cleaning process in one go. All the values have defaults so it can be called even without parameters.\n",
        "\n",
        "Important parameters are\n",
        "<br>agument_class_simple: which simply duplicates minority class.\n",
        "<br>transform_columnWise: which transaform each feature independently.\n",
        "<br>augment_with_odd_even: applies our very first class. Class to augment are hardcoded method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SGrMpYtdLolf"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "def data_prep(\n",
        "    ecg_train: np.ndarray = ecg_train,\n",
        "    ecg_validation: np.ndarray = ecg_validation,\n",
        "    swap_train_vld: bool = True,\n",
        "    augment_classes_simple: dict = {2: 290, 3: 200, 4: 500},\n",
        "    transform_columnWise: bool = True,\n",
        "    augment_with_odd_even: bool = False,\n",
        "    verbose: bool = True\n",
        "    ):\n",
        "\n",
        "\n",
        "  #swap set\n",
        "  #Checking if train and validation was swapped by mistake\n",
        "  if ecg_train.shape[0] > ecg_validation.shape[0]:\n",
        "      temp = ecg_train\n",
        "      ecg_train = ecg_validation\n",
        "      ecg_validation = temp\n",
        "  #Swap vld train\n",
        "  if swap_train_vld:\n",
        "    swapTrainvld = True\n",
        "    temp = ecg_train\n",
        "    ecg_train = ecg_validation\n",
        "    ecg_validation = temp\n",
        "\n",
        "  #feature label split\n",
        "  ecg_train_data = ecg_train[ : ,1:]\n",
        "  ecg_train_labels = ecg_train[:,0:1]\n",
        "  ecg_validation_data = ecg_validation[ : ,1:]\n",
        "  ecg_validation_labels = ecg_validation[:,0:1]\n",
        "\n",
        "  #reassign\n",
        "  X_trn = ecg_train_data\n",
        "  y_trn = ecg_train_labels\n",
        "  X_vld = ecg_validation[:, 1:]\n",
        "  y_vld = ecg_validation_labels\n",
        "  if verbose: printInfo(X_trn, y_trn, \"Initial train\")\n",
        "  if verbose: printInfo(X_trn, y_trn, \"Initial validation\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Augment with odd_even\n",
        "  if augment_with_odd_even:\n",
        "    if verbose: printInfo(X_trn, y_trn, \"Before augment_with_odd_even\")\n",
        "    y_trn = y_trn.squeeze()\n",
        "    y_trn = y_trn.astype(np.int32)\n",
        "    X_trn, y_trn = augment_minority_with_noise_and_smoothing(X_trn, y_trn)\n",
        "    if verbose: printInfo(X_trn, y_trn, \"After augment_with_odd_even\")\n",
        "\n",
        "  #Augment simple\n",
        "  if augment_classes_simple:\n",
        "    if verbose: printInfo(X_trn, y_trn, \"Before augment_classes_simple\")\n",
        "    X_aug, y_aug = [], []\n",
        "    aug_info = {2: 290, 3: 200, 4: 500}\n",
        "\n",
        "    for cls, count in aug_info.items():\n",
        "        x_new, y_new = augment_class(X_trn, y_trn, cls, count)\n",
        "        X_aug.append(x_new)\n",
        "        y_aug.append(y_new)\n",
        "\n",
        "    X_trn = np.concatenate([X_trn] + X_aug)\n",
        "    y_trn = np.concatenate([y_trn] + y_aug)\n",
        "    if verbose: printInfo(X_trn, y_trn, \"After augment_classes_simple\")\n",
        "\n",
        "    #Standardization\n",
        "\n",
        "    if transform_columnWise:\n",
        "      X_trn = scaler.fit_transform(X_trn)\n",
        "      X_vld = scaler.transform(X_vld)\n",
        "    else:\n",
        "      X_trn  = X_trn.T\n",
        "      X_vld = X_vld.T\n",
        "      X_trn = scaler.fit_transform(X_trn)\n",
        "      X_vld = scaler.fit_transform(X_vld)\n",
        "      X_trn  = X_trn.T\n",
        "      X_vld = X_vld.T\n",
        "\n",
        "    # Reshape for TCN [batch, time_steps, features]\n",
        "    X_trn = X_trn.reshape(X_trn.shape[0], X_trn.shape[1], 1)\n",
        "    X_vld = X_vld.reshape(X_vld.shape[0], X_vld.shape[1], 1)\n",
        "\n",
        "    if verbose: printInfo(X_trn, y_trn, \"Before One hot train\")\n",
        "    if verbose: printInfo(X_vld, y_vld, \"Before One hot vald\")\n",
        "    num_classes = 5\n",
        "    y_trn_oh = to_categorical(y_trn, num_classes)\n",
        "    y_vld_oh = to_categorical(y_vld, num_classes)\n",
        "    if verbose: printInfo(X_trn, y_trn_oh, \"After One hot train\")\n",
        "    if verbose: printInfo(X_vld, y_vld_oh, \"After One hot vald\")\n",
        "\n",
        "    return X_trn, y_trn_oh, X_vld, y_vld_oh, y_trn, y_vld\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# returns:\n",
        "#X_trn, y_trn_oh, X_vld, y_vld_oh, y_trn, y_vld\n",
        "#X_trn, y_trn_oh, X_vld, y_vld_oh, _, _ = data_prep()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnGtAmPoEMHr",
        "outputId": "bf4d1815-de9c-4fc4-8a31-7e2f8bc164b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Y is not one hot encoded here\n",
            "Initial train\n",
            "Types: \n",
            "\tX_trn type = <class 'numpy.ndarray'>, y_trn type = <class 'numpy.ndarray'>\n",
            "Shapes: \n",
            "\tX_trn.shape = (1500, 140), y_trn.shape = (1500, 1)\n",
            "Y unique plus counts\n",
            "\tClass = 0.0, counts = 781\n",
            "\tClass = 1.0, counts = 590\n",
            "\tClass = 2.0, counts = 43\n",
            "\tClass = 3.0, counts = 75\n",
            "\tClass = 4.0, counts = 11\n",
            "\n",
            "\n",
            "Y is not one hot encoded here\n",
            "Initial validation\n",
            "Types: \n",
            "\tX_trn type = <class 'numpy.ndarray'>, y_trn type = <class 'numpy.ndarray'>\n",
            "Shapes: \n",
            "\tX_trn.shape = (1500, 140), y_trn.shape = (1500, 1)\n",
            "Y unique plus counts\n",
            "\tClass = 0.0, counts = 781\n",
            "\tClass = 1.0, counts = 590\n",
            "\tClass = 2.0, counts = 43\n",
            "\tClass = 3.0, counts = 75\n",
            "\tClass = 4.0, counts = 11\n",
            "\n",
            "\n",
            "Y is not one hot encoded here\n",
            "Before augment_with_odd_even\n",
            "Types: \n",
            "\tX_trn type = <class 'numpy.ndarray'>, y_trn type = <class 'numpy.ndarray'>\n",
            "Shapes: \n",
            "\tX_trn.shape = (1500, 140), y_trn.shape = (1500, 1)\n",
            "Y unique plus counts\n",
            "\tClass = 0.0, counts = 781\n",
            "\tClass = 1.0, counts = 590\n",
            "\tClass = 2.0, counts = 43\n",
            "\tClass = 3.0, counts = 75\n",
            "\tClass = 4.0, counts = 11\n",
            "\n",
            "\n",
            "Y is not one hot encoded here\n",
            "After augment_with_odd_even\n",
            "Types: \n",
            "\tX_trn type = <class 'numpy.ndarray'>, y_trn type = <class 'numpy.ndarray'>\n",
            "Shapes: \n",
            "\tX_trn.shape = (4416, 140), y_trn.shape = (4416,)\n",
            "Y unique plus counts\n",
            "\tClass = 0, counts = 781\n",
            "\tClass = 1, counts = 590\n",
            "\tClass = 2, counts = 1015\n",
            "\tClass = 3, counts = 1015\n",
            "\tClass = 4, counts = 1015\n",
            "\n",
            "\n",
            "Y is not one hot encoded here\n",
            "Before augment_classes_simple\n",
            "Types: \n",
            "\tX_trn type = <class 'numpy.ndarray'>, y_trn type = <class 'numpy.ndarray'>\n",
            "Shapes: \n",
            "\tX_trn.shape = (4416, 140), y_trn.shape = (4416,)\n",
            "Y unique plus counts\n",
            "\tClass = 0, counts = 781\n",
            "\tClass = 1, counts = 590\n",
            "\tClass = 2, counts = 1015\n",
            "\tClass = 3, counts = 1015\n",
            "\tClass = 4, counts = 1015\n",
            "\n",
            "\n",
            "Y is not one hot encoded here\n",
            "After augment_classes_simple\n",
            "Types: \n",
            "\tX_trn type = <class 'numpy.ndarray'>, y_trn type = <class 'numpy.ndarray'>\n",
            "Shapes: \n",
            "\tX_trn.shape = (5406, 140), y_trn.shape = (5406,)\n",
            "Y unique plus counts\n",
            "\tClass = 0, counts = 781\n",
            "\tClass = 1, counts = 590\n",
            "\tClass = 2, counts = 1305\n",
            "\tClass = 3, counts = 1215\n",
            "\tClass = 4, counts = 1515\n",
            "\n",
            "\n",
            "Y is not one hot encoded here\n",
            "Before One hot train\n",
            "Types: \n",
            "\tX_trn type = <class 'numpy.ndarray'>, y_trn type = <class 'numpy.ndarray'>\n",
            "Shapes: \n",
            "\tX_trn.shape = (5406, 140, 1), y_trn.shape = (5406,)\n",
            "Y unique plus counts\n",
            "\tClass = 0, counts = 781\n",
            "\tClass = 1, counts = 590\n",
            "\tClass = 2, counts = 1305\n",
            "\tClass = 3, counts = 1215\n",
            "\tClass = 4, counts = 1515\n",
            "\n",
            "\n",
            "Y is not one hot encoded here\n",
            "Before One hot vald\n",
            "Types: \n",
            "\tX_trn type = <class 'numpy.ndarray'>, y_trn type = <class 'numpy.ndarray'>\n",
            "Shapes: \n",
            "\tX_trn.shape = (500, 140, 1), y_trn.shape = (500, 1)\n",
            "Y unique plus counts\n",
            "\tClass = 0.0, counts = 292\n",
            "\tClass = 1.0, counts = 177\n",
            "\tClass = 2.0, counts = 10\n",
            "\tClass = 3.0, counts = 19\n",
            "\tClass = 4.0, counts = 2\n",
            "\n",
            "\n",
            "After One hot train\n",
            "Types: \n",
            "\tX_trn type = <class 'numpy.ndarray'>, y_trn type = <class 'numpy.ndarray'>\n",
            "Shapes: \n",
            "\tX_trn.shape = (5406, 140, 1), y_trn.shape = (5406, 5)\n",
            "Y unique plus counts\n",
            "\tClass = 0, counts = 781.0\n",
            "\tClass = 1, counts = 590.0\n",
            "\tClass = 2, counts = 1305.0\n",
            "\tClass = 3, counts = 1215.0\n",
            "\tClass = 4, counts = 1515.0\n",
            "\n",
            "\n",
            "After One hot vald\n",
            "Types: \n",
            "\tX_trn type = <class 'numpy.ndarray'>, y_trn type = <class 'numpy.ndarray'>\n",
            "Shapes: \n",
            "\tX_trn.shape = (500, 140, 1), y_trn.shape = (500, 5)\n",
            "Y unique plus counts\n",
            "\tClass = 0, counts = 292.0\n",
            "\tClass = 1, counts = 177.0\n",
            "\tClass = 2, counts = 10.0\n",
            "\tClass = 3, counts = 19.0\n",
            "\tClass = 4, counts = 2.0\n"
          ]
        }
      ],
      "source": [
        "X_trn, y_trn_oh, X_vld, y_vld_oh, _, _ = data_prep(\n",
        "    ecg_train = ecg_train,\n",
        "    ecg_validation = ecg_validation,\n",
        "    swap_train_vld = swapTrainvld, #set this manaully to false to see paper's score\n",
        "    augment_classes_simple = {3: 100},#{0: 250, 1: 250, 2: 260, 3: 400, 4: 1000}, # increase minority class\n",
        "    transform_columnWise = True,\n",
        "    augment_with_odd_even = True, #increasing minority classes\n",
        "    verbose = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBxpcrQXl8J0"
      },
      "source": [
        "Using relu instead of sigmoid, increasing layers to 32 from 16. Changing dropout to 0.3 to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "npJcltJAp7SH"
      },
      "outputs": [],
      "source": [
        "def build_tcn_model(input_shape, num_classes):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    filter_sizes = [2, 4, 6, 8]\n",
        "\n",
        "    for i, fs in enumerate(filter_sizes):\n",
        "        dilation_rate = 2 ** i\n",
        "        x_prev = x\n",
        "        x = layers.Conv1D(32, fs, padding='causal', dilation_rate=dilation_rate, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.SpatialDropout1D(0.3)(x)\n",
        "        x = layers.Add()([x, x_prev]) if x.shape[-1] == x_prev.shape[-1] else x\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTqGb6G_uOJg",
        "outputId": "f37e58e4-2b87-4761-94ee-9ed826cdb1bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target f1 for early stopping = 0.85\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# Adding weights to minority class.\n",
        "class_weight_dict = {\n",
        "    0: 50.0,\n",
        "    1: 40.0,\n",
        "    2: 30.5,\n",
        "    3: 40.0,\n",
        "    4: 60.0\n",
        "}\n",
        "\n",
        "# Instead of relying on accuracy, we directly calculate validation f1 score per epoch and print it.\n",
        "# If model reachs our desired socre it stops training.\n",
        "class F1EarlyStopping(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, val_data, target_f1=0.90):\n",
        "        super().__init__()\n",
        "        self.X_val, self.y_val = val_data\n",
        "        self.target_f1 = target_f1\n",
        "        self.best_f1 = 0\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        y_true = np.argmax(self.y_val, axis=1)\n",
        "\n",
        "        current_f1 = f1_score(y_true, y_pred_classes, average='macro')\n",
        "        logs['val_f1_score'] = current_f1\n",
        "\n",
        "        print(f\" â€” val_f1_score: {current_f1:.4f}\", end='')\n",
        "\n",
        "\n",
        "        if current_f1 >= self.target_f1:\n",
        "            print(f\"\\nEarly stopping - F1 reached {current_f1:.4f} (target: {self.target_f1})\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "        if current_f1 > self.best_f1:\n",
        "            self.best_f1 = current_f1\n",
        "            print(f\"\\tEpoch = {epoch}\")\n",
        "            print(\" (new best)\", end='')\n",
        "        print()\n",
        "# Defining early stopping since we may not get same performance even after finding\n",
        "# epoch number because shuffle = True when running the model.\n",
        "if swapTrainvld:\n",
        "  setTargetF1 = 0.85\n",
        "else:\n",
        "  setTargetF1 = 0.65\n",
        "f1_callback = F1EarlyStopping(\n",
        "    val_data=(X_vld, y_vld_oh),\n",
        "    target_f1=setTargetF1\n",
        ")\n",
        "print(f\"Target f1 for early stopping = {setTargetF1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBA-otZBtw-c",
        "outputId": "9a1630ab-348f-47ce-dfc5-9d8f0d9aa39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " â€” val_f1_score: 0.2108\tEpoch = 0\n",
            " (new best)\n",
            " â€” val_f1_score: 0.4999\tEpoch = 1\n",
            " (new best)\n",
            " â€” val_f1_score: 0.4930\n",
            " â€” val_f1_score: 0.6190\tEpoch = 3\n",
            " (new best)\n",
            " â€” val_f1_score: 0.5950\n",
            " â€” val_f1_score: 0.6810\tEpoch = 5\n",
            " (new best)\n",
            " â€” val_f1_score: 0.6954\tEpoch = 6\n",
            " (new best)\n",
            " â€” val_f1_score: 0.7010\tEpoch = 7\n",
            " (new best)\n",
            " â€” val_f1_score: 0.7556\tEpoch = 8\n",
            " (new best)\n",
            " â€” val_f1_score: 0.7783\tEpoch = 9\n",
            " (new best)\n"
          ]
        }
      ],
      "source": [
        "model = build_tcn_model(X_trn.shape[1:], 5)#num_classes)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    X_trn, y_trn_oh,\n",
        "    epochs=400,\n",
        "    batch_size=24,\n",
        "    validation_data=(X_vld, y_vld_oh),\n",
        "    callbacks=[f1_callback],\n",
        "    verbose=0, # Because of hardware limitions, verbose has been turned off to prevent PC from hanging.\n",
        "    class_weight = class_weight_dict\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM4rbl89wzOS"
      },
      "outputs": [],
      "source": [
        "y_pred_probs = model.predict(X_vld)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_vld_oh, axis=1)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"F1 Score (macro):\", f1_score(y_true, y_pred, average='macro'))\n",
        "print(\"\\n\", classification_report(y_true, y_pred))\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4vM1_-VQM8j"
      },
      "source": [
        "Based on results and based on KNN classifier from earlier project where we had class imbalance we can trust models instinct, ie first preference for class 0 and 1.\n",
        "In other words, if the model says that the given sample is class 0 and 1 we can trust blindly. If model says that given sample is 2-3 then we cannot blindly trust model as it didn't see enough data for class 2-3 and almost nothing for 4. But model did see some samples of minority classes so it also means it learned something."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGMOsyc-gJC-"
      },
      "outputs": [],
      "source": [
        "X_trn, _, X_vld, _, _, y_vld =  data_prep(\n",
        "    ecg_train = ecg_train,\n",
        "    ecg_validation = ecg_validation,\n",
        "    swap_train_vld = swapTrainvld,\n",
        "    augment_classes_simple = {3: 100}, # This augmentation wasn't working that great so we only doing it for class one.\n",
        "    transform_columnWise = True,\n",
        "    augment_with_odd_even = True, #increasing minority classes with repeatition\n",
        "    verbose = False\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cZAfN-Towzk"
      },
      "source": [
        "We try to pick random samples from predicted samples. Idea is that since model is good with class 1 and 2 we leave it alone. Since class 2 and 3 are minority and 4 is extreme minority, we randomly pick data samples class 2 and 3 since they are in middle and assign them manually to other classes. Since we need some pattern, we convert probabilities to ranking.\n",
        "So every sample will now have ranking of class preferences they would like to be in.\n",
        "<br>For example\n",
        "<br>Class idx (0,1,2,3,4)\n",
        "<br>Sample1 = (1,2,0,4,3)<br>\n",
        "Ranking is reversed so lowest ranking = highest probability and vica-versa.<br>\n",
        "This sample will be predicted as class 2.\n",
        "\n",
        "Idea is that, if model is good in identifying samples and if model says given sample is 0,1 we can trust models 1st instinct which is mostly right.\n",
        "If it says it's 2 or 3 we can't trust it's 1st instinct/preference. But we also don't know if we can trust it's 2nd, 3rd or 4th instinct/preference when it predicts 2 or 3. Also we won't know which know which class to override from\n",
        "\n",
        "So we use nested for loops to find combination of class to overRide samples with, and sample's preference for that class. This might be similar to threshold tuning on validation set, but instead we are doing preference based post processing of results.\n",
        "Since softmax will try to make incorrect class probabilities lower, we convert them into rank and find prefences of model.\n",
        "\n",
        "Theory is that, since we are being told, before speaking in important situation, we should not say 1st,2nd of 3rd thing that comes to our mind instead 4th one. Because most of the times anything after 1st thinking is relatively more rational thinking then 1st one.\n",
        "If we apply same logic to model, then we want to find preference/thinking of that model and class to apply those perferences to.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0GulQ6K7uGP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zm7MN_vYZfF"
      },
      "outputs": [],
      "source": [
        "def overRidePredictLoop (X_vld, y_vld, class_overRide = 3, sample_preference = 3, return_value = False):\n",
        "  # Regular predictions\n",
        "  y_pred_probs = model.predict(X_vld)\n",
        "  y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "  y_pred_original = y_pred.copy()\n",
        "\n",
        "  # converting probabilities into ranking\n",
        "  rankings = np.argsort(-y_pred_probs, axis=1)\n",
        "\n",
        "  # Appling target override. If any sample is predicted as 2 or 3, we loop through their\n",
        "  # preferences for each class and class to overRide that sample. Here we have possible of\n",
        "  # 16 combinations 4 * 4 since we are not considering override on class 0. If the model\n",
        "  # is performaning perfectly, this method will report 0.00 increase in performance, meaning there is no\n",
        "  # possible combination of class_overRide amd sample_preference which can further increase it's score.\n",
        "  # However these are still overRidden scores for f1 and not something that model has learned.\n",
        "\n",
        "  eligible_indices = [\n",
        "      i for i in range(len(y_pred))\n",
        "      if y_pred[i] in [2, 3] and rankings[i][class_overRide] == sample_preference\n",
        "  ]\n",
        "  eligible_indices = np.array(eligible_indices, dtype=int)\n",
        "\n",
        "  np.random.seed(42)\n",
        "  override_indices = np.random.choice(eligible_indices, size=min(4000, len(eligible_indices)), replace=False)\n",
        "\n",
        "  y_pred[eligible_indices] = class_overRide\n",
        "\n",
        "  overRide = f1_score(y_vld, y_pred, average='macro')\n",
        "  original = f1_score(y_vld, y_pred_original, average='macro')\n",
        "  result = overRide - original\n",
        "  if return_value:\n",
        "    return result\n",
        "  else:\n",
        "    # Evaluate new predictions\n",
        "    print(\"After Targeted Override \")\n",
        "    print(confusion_matrix(y_vld, y_pred))\n",
        "    print(classification_report(y_vld, y_pred, digits=4))\n",
        "    print(\"\\nOriginal Macro F1:\", f1_score(y_vld, y_pred_original, average='macro'))\n",
        "    print(\"\\n\\n\\n\")\n",
        "    print(\"ðŸ”»NOT MODELS PERFORMANCE REAL PERFORMANCE \")\n",
        "    print(\"Override Macro F1 Score:\", f1_score(y_vld, y_pred, average='macro'))\n",
        "    print(\"ðŸ”ºNOT MODELS PERFORMANCE REAL PERFORMANCE \")\n",
        "    if result < 0:\n",
        "      print(f\"Performance drop = {result}\")\n",
        "    else:\n",
        "      print(f\"Performance increase = {result}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#print(overRidePredictLoop(X_vld, y_vld, class_overRide=3, sample_preference = 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_vOvTy01OHw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def findBestOverRide(X_vld, y_vld):\n",
        "  class_overRide = 0\n",
        "  sample_preference= 0\n",
        "  best_f1 = -100\n",
        "  for i in range(1,5):\n",
        "    for j in range(1,5):\n",
        "      current_f1 = overRidePredictLoop(X_vld, y_vld, class_overRide = i, sample_preference = j, return_value=True)\n",
        "      if current_f1 > best_f1:\n",
        "        print(f\"curent difference = {current_f1}\")\n",
        "        best_f1 = current_f1\n",
        "        class_overRide = i\n",
        "        sample_preference = j\n",
        "        print(f\"New best f1, increase of  = {best_f1}, for class_override = {i}, and sample_preference = {j}\")\n",
        "  return class_overRide, sample_preference\n",
        "\n",
        "\n",
        "class_overRide, sample_preference = findBestOverRide(X_vld, y_vld)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBfg-1NrVW1D"
      },
      "outputs": [],
      "source": [
        "#final over ride\n",
        "overRidePredictLoop(X_vld, y_vld, class_overRide = class_overRide, sample_preference = sample_preference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYbub3js_BLb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsZnpPdFpn5"
      },
      "source": [
        "This was implementation of our Method A from  https://pmc.ncbi.nlm.nih.gov/articles/PMC9920651/ paper .\n",
        "Their original score was\n",
        "f1 = 84%, accuracy = 96% with data agumentation on minority classes like 2,3,4.\n",
        "\n",
        "Things used from papers were concecpts of TCN model, data augmentation, class weights.\n",
        "\n",
        "My condtributions: My contribution to this was\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**DataAugmentation:** Instead of using standard data augmentation, we used noise based plus minus data augmentation methods along with odd even smoothing which not only creates more samples for minority class, but those extra samples are also smoothed out, which helps might have helped in capturing underlying patters of minority classes and contributed in increasing performance.\n",
        "\n",
        "There was 1% F1 increase on validation's original socre when we augmented class 3 aswell along with 2 and 4. Along with that our model converged relatively early then before.\n",
        "\n",
        "**overRidePredictions()** which increase performance artificaly if it can for the given validation set by finding the best combination of class to overRide on and preference to consider. This resulted in 0.77% increase in artifically bosted performance.\n",
        "\n",
        "Apart from that, training on raw data without any augmentations gets around 4.00% f1 score boost from original data set. However for that, we must train it on raw data without any class weights or data augmentation.\n",
        "One limitation of this is that while it doesn't decrease f1 because it excludes class 0 and 1, and only considers 2 and 3, it also means that currently if model predicts minority samples are class 0 or 1, this method can't work with that.\n",
        "\n",
        "Another limitations of this artifical boosting method is that while working further on this, or in real life cases, we must ensure that test data set will follow roughly same class distribution as it's seen in training data sets. One way to achieve these would be to gather samples from differnt places and have them predicted at once in batch. That might be the only way to get reliable results from these artifical boost.\n",
        "\n",
        "In real life implementation for example in hospitals, they must wait for set period of time until there are atleast enough samples to test from when we would be sure that those samples might follow natural pattern that was in training data set. Wether these artifical boosts are okay in machine learning world or not might be in grey area. But there is one thing that, it might open up new way of working with model's flaw. Instead of perfecting model, we work with their flaws to get best performance for each class.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "class_weights_original_0 = {0: 0.1459, 1: 0.1041}\n",
        "class_weights_original_1 = {0: 0.884, 1: 0.157}\n",
        "class_weights_original_2 = {0: 1.97,   1: 20.60}\n",
        "class_weights_original_3 = {0: 10.48,  1: 25.12}\n",
        "# for binary\n",
        "y_trn_0 = np.argmax(y_trn_0, axis = 1).reshape(-1,1)\n",
        "y_trn_1 = np.argmax(y_trn_1, axis = 1).reshape(-1,1)\n",
        "y_trn_2 = np.argmax(y_trn_2, axis = 1).reshape(-1,1)\n",
        "y_trn_3 = np.argmax(y_trn_3, axis = 1).reshape(-1,1)\n",
        "\n",
        "\n",
        "def build_tcn_model(input_shape, num_classes=2):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    filter_sizes = [2, 4, 6, 8]\n",
        "\n",
        "    for i, fs in enumerate(filter_sizes):\n",
        "        dilation_rate = 2 ** i\n",
        "        x_prev = x\n",
        "        x = layers.Conv1D(16, fs, padding='causal', dilation_rate=dilation_rate, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.SpatialDropout1D(0.1)(x)\n",
        "        x = layers.Add()([x, x_prev]) if x.shape[-1] == x_prev.shape[-1] else x\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    #outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def get_class_weights(y):\n",
        "    y_integers = np.argmax(y, axis=1)\n",
        "    weights = class_weight.compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_integers),\n",
        "        y=y_integers\n",
        "    )\n",
        "    return dict(enumerate(weights))\n",
        "\n",
        "print(\"Model 0\")\n",
        "model0 = build_tcn_model(X_trn_0.shape[1:], num_classes=2)\n",
        "model0.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "class_weights_0 = get_class_weights(y_trn_0)\n",
        "history0 = model0.fit(\n",
        "    X_trn_0, y_trn_0,\n",
        "    epochs=19,\n",
        "    batch_size=20,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        "    class_weight= class_weights_original_0\n",
        ")\n",
        "\n",
        "print(\"Model 1\")\n",
        "model1 = build_tcn_model(X_trn_1.shape[1:], num_classes=2)\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "class_weights_1 = get_class_weights(y_trn_1)\n",
        "history1 = model1.fit(\n",
        "    X_trn_1, y_trn_1,\n",
        "    epochs=12,\n",
        "    batch_size=20,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        "    class_weight=class_weights_original_1\n",
        ")\n",
        "\n",
        "print(\"Model 2\")\n",
        "model2 = build_tcn_model(X_trn_2.shape[1:], num_classes=2)\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "class_weights_2 = get_class_weights(y_trn_2)\n",
        "history2 = model2.fit(\n",
        "    X_trn_2, y_trn_2,\n",
        "    epochs=50,\n",
        "    batch_size=10,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        "    class_weight=class_weights_original_2\n",
        ")\n",
        "\n",
        "print(\"Model 3\")\n",
        "model3 = build_tcn_model(X_trn_3.shape[1:], num_classes=2)\n",
        "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "class_weights_3 = get_class_weights(y_trn_3)\n",
        "history3 = model3.fit(\n",
        "    X_trn_3, y_trn_3,\n",
        "    epochs=50,\n",
        "    batch_size=5,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        "    class_weight=class_weights_original_3\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "def Splitter(X_trn, y_trn):\n",
        "    features = X_trn.shape[1]\n",
        "\n",
        "    X_0, y_0 = [], []\n",
        "    X_1, y_1 = [], []\n",
        "    X_3, y_3 = [], []\n",
        "    X_2, y_2 = [], []\n",
        "\n",
        "    for i in range(len(X_trn)):\n",
        "        x = X_trn[i]\n",
        "        y = y_trn[i]\n",
        "\n",
        "        # Stage 0: 0 vs 1,2,3,4\n",
        "        X_0.append(x)\n",
        "        y_0.append(0 if y == 0 else 1)\n",
        "\n",
        "        if y != 0:\n",
        "            # Stage 1: 1 vs 2,3,4\n",
        "            X_1.append(x)\n",
        "            y_1.append(0 if y == 1 else 1)\n",
        "\n",
        "            if y != 1:\n",
        "                # Stage 3: 3 vs 2,4\n",
        "                if y in [2, 3, 4]:\n",
        "                    X_3.append(x)\n",
        "                    y_3.append(0 if y == 3 else 1)\n",
        "\n",
        "                # Stage 2: 2 vs 4 (only if passed stage 3 as not class 3)\n",
        "                if y in [2, 4] and y != 3:\n",
        "                    X_2.append(x)\n",
        "                    y_2.append(0 if y == 2 else 1)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    return (\n",
        "        np.array(X_0), np.array(y_0),\n",
        "        np.array(X_1), np.array(y_1),\n",
        "        np.array(X_3), np.array(y_3),\n",
        "        np.array(X_2), np.array(y_2)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def predictmerger()\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
